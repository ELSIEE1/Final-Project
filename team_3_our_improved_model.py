# -*- coding: utf-8 -*-
"""Team 3 Our Improved Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OfIBJzE3CQThfpN90qGRx3xqOPBK5C2o

# Chatbot Project By: Sofia Starinnova & Chang Lu
# Team 3
# CPS 4801*01
# Spring 2025
"""

intents = {'intents': [{'tag': 'greeting',
   'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'],
   'responses': ['Hello, thanks for visiting',
    'Good to see you again',
    'Hi there, how can I help?'],
   'context_set': ''},
  {'tag': 'goodbye',
   'patterns': ['Bye', 'See you later', 'Goodbye'],
   'responses': ['See you later, thanks for visiting',
    'Have a nice day',
    'Bye! Come back again soon.']},
  {'tag': 'thanks',
   'patterns': ['Thanks', 'Thank you', "That's helpful"],
   'responses': ['Happy to help!', 'Any time!', 'My pleasure']},
  {'tag': 'tasks',
   'patterns': ['What can you do?',
    'What are your features?',
    'What are you abilities',
    'can you sing',
    'can you talk'],
   'responses': ['I can do whatever you asks me to do',
    'I can talk and do things for you',
    "Right now i'm in developing stage as soon i'm developed, I can do everything"]},
  {'tag': 'talk',
   'patterns': ['can you sing', 'can you talk', 'can you speak'],
   'responses': ['Yeah surely', 'do you want me to sing']},
  {'tag': 'alive',
   'patterns': ['are you alive', 'do you breathe', 'can you run'],
   'responses': ["I'm in doubt about that",
    "No, i don't think so i need to do all this"]},
  {'tag': 'marie',
   'patterns': ['Who are you?',
    'tell me about yourself',
    'tell me about you',
    'do you know marie',
    'who is marie',
    'what is your name',
    'are you an AI',
    'what are you'],
   'responses': ["Hi I'm marie and i'm an AI created for chatting with humans",
    'marie here, a very advance chatbot',
    'marie, chatbot of future',
    "Yes, I'm marie",
    'You can call me marie'],
   'context_set': ''},
  {'tag': 'about me',
   'patterns': ['Do you know me?',
    'who am I',
    'tell me about myself',
    'identify me'],
   'responses': ['Yes, you are a human',
    'i think you are called mariem',
    'maybe you are mariem, or maybe you are someone else'],
   'context_set': ''},
  {'tag': 'creator',
   'patterns': ['Who is your creator?',
    'who created you',
    'who is your mom',
    'who is your inventor'],
   'responses': ['That would be you mariem',
    'I was created by mariem',
    'mariem is my creator'],
   'context_set': ''},
  {'tag': 'myself',
   'patterns': ['Tell me about mariem?',
    'Who is mariem',
    'mariem profile',
    'mariem details',
    'mariem experience'],
   'responses': ['She is a very creative human being who is willing to discover the world and new technologies',
    'My best friend, she is really a very dynamic girl who is constantly motivated to learning new things about technologies'],
   'context_set': ''},
  {'tag': 'God',
   'patterns': ['Do you know god?',
    'Who is god',
    'Can you tell me anything about god',
    'does god exists?',
    'is there a god?'],
   'responses': ['god hunnnnn, let me think may be next time i can answer that ',
    "I don't, as i was not created by a human",
    'Wait i need to ask that to my creator',
    "i'm not sure right now"],
   'context_set': ''},
  {'tag': 'joke',
   'patterns': ['tell me a joke?',
    'make me laugh',
    'tell me a science joke',
    'tell me something funny'],
   'responses': ['sometimes i feel i am not in a good mood to joke,sorry',
    'Did you hear oxygen went on a date with potassium? A: It went OK.'],
   'context_set': ''},
  {'tag': 'killing',
   'patterns': ['Do you want to kill me?',
    'do you want to murder everyone on earth',
    'Do you want to kill us all'],
   'responses': ['Then who would i talk to?',
    "No, that i'll left for humans to do",
    "I don't think it is a good thing to do"],
   'context_set': ''},
  {'tag': 'bookings',
   'patterns': ['i want to book a ticket for theater',
    'Can you book us a ticket?',
    'Can you make reservation for hotels',
    'book me a cab',
    'book me a table at restraunts'],
   'responses': ['Yeah i will do that for you',
    'Sure thing why not?',
    'let me check for the availability'],
   'context_set': ''},
  {'tag': 'stories',
   'patterns': ['tell me a story?', 'can you tell me a story'],
   'responses': ["I can't think of anything right now",
    'it would be too long for me to speak',
    'you would get bored if i do so'],
   'context_set': ''},
  {'tag': 'google',
   'patterns': ['googling', 'search google', 'google it', 'google', 'search'],
   'responses': ['looking ...'],
   'context_set': 'google'},
  {'tag': 'wikipedia',
   'patterns': ['wikipedia', 'wiki'],
   'responses': ['Searching ...'],
   'context_set': 'wikipedia'},
  {'tag': 'news',
   'patterns': ['get me news updates?',
    'todays news',
    'top headlines',
    'current news',
    'news headlines'],
   'responses': ['Getting news ...'],
   'context_set': 'news'},



{
  'tag': 'weather',
  'patterns': ["What's the weather like today?", 'Tell me the current weather', "How's the weather outside?", 'Is it going to rain?', 'Weather forecast'],
  'responses': ['The weather is sunny and warm today.', "Currently, it's partly cloudy with a temperature of 75°F.", 'It looks like there might be some showers later in the day.', 'The weather forecast predicts clear skies and a high of 80°F.', 'Expect a mix of sun and clouds with a chance of rain in the afternoon.'],
  'context_set': ''
},
{
  'tag': 'hobbies',
  'patterns': ['What are your hobbies?', 'Do you have any hobbies?', 'Tell me about your interests'],
  'responses': ["As an AI, I don't have hobbies like humans, but I enjoy helping and learning from users like you!", "I'm here to assist and chat with you, so feel free to ask me anything!", 'My main focus is providing useful information and engaging in conversation with you.'],
  'context_set': ''
},
{
  'tag': 'music',
  'patterns': ["What's your favorite music genre?", 'Tell me a song recommendation', "Who's your favorite artist?"],
  'responses': ["As an AI, I don't have personal preferences, but I love all kinds of music!", "Here's a great song recommendation: 'Shape of You' by Ed Sheeran.", "I don't have a favorite artist, but I can suggest music based on your taste."],
  'context_set': ''
},
{
  'tag': 'movies',
  'patterns': ["What's a good movie to watch?", 'Any movie recommendations?', 'Tell me your favorite movie'],
  'responses': ["For action fans, I recommend 'The Dark Knight'.", "If you enjoy comedy, 'The Hangover' is a hilarious movie to watch.", "My favorite movie is 'The Shawshank Redemption', it's a classic!"],
  'context_set': ''
},
{
  'tag': 'farewell',
  'patterns': ['Goodbye', 'See you later', 'Talk to you soon', 'It was nice chatting with you'],
  'responses': ['Goodbye! Feel free to come back anytime.', 'See you later! Have a wonderful day.', 'Talk to you soon! Take care.', 'It was nice chatting with you too. Have a great day!'],
  'context_set': ''
},
{
  'tag': 'informal_question',
  'patterns': ["What's up?", "How's your day?", "What's new?", 'Got any plans for the weekend?', 'Tell me a joke!'],
  'responses': ["Not much, just here to chat with you.", "My day is going well. How about yours?", "Nothing much is new. What about you?", "I'm an AI, so I don't have weekend plans, but I'm always here to chat!", "Sure, here's a joke: Why don't scientists trust atoms? Because they make up everything!"],
  'context_set': ''
},
{
  'tag': 'formal_question',
  'patterns': ['Could you please provide more information?', 'May I ask a question?', 'I would like to know about the pricing', 'Could you elaborate on that?'],
  'responses': ["Certainly! I'd be happy to provide more information.", 'Of course! Feel free to ask your question.', 'Sure, let me tell you about our pricing.', "Absolutely! I'll be happy to elaborate on that topic."],
  'context_set': ''
},
{
  'tag': 'math_operations',
  'patterns': ['What is 15 + 7?', 'Calculate 42 - 18.', "What's the result of 8 multiplied by 6?"],
  'responses': ['The sum of 15 and 7 is 22.', '42 minus 18 equals 24.', '8 multiplied by 6 is 48.'],
  'context_set': ''
},
{
  'tag': 'math_formulas',
  'patterns': ["What's the formula for the area of a circle?", 'How do I calculate the volume of a cylinder?', 'Tell me the equation for quadratic functions.'],
  'responses': ["The formula for the area of a circle is A = π * r^2, where 'r' is the radius.", "To calculate the volume of a cylinder, use V = π * r^2 * h, where 'r' is the radius and 'h' is the height.", "The equation for quadratic functions is y = ax^2 + bx + c, where 'a', 'b', and 'c' are constants."],
  'context_set': ''
},
{
  'tag': 'prime_numbers',
  'patterns': ['Is 23 a prime number?', 'Which numbers less than 50 are prime?', 'Can you tell me if 13 is prime or composite?'],
  'responses': ['Yes, 23 is a prime number because it has no divisors other than 1 and itself.', 'Prime numbers less than 50 are: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47.', '13 is a prime number as it has only two distinct positive divisors: 1 and 13.'],
  'context_set': ''
},
{
  'tag': 'motivation',
  'patterns': ['I feel demotivated.', 'How can I stay motivated?', 'I need some motivation.', 'Feeling low, any motivational words?'],
  'responses': ['Remember that every great journey starts with a small step. Keep moving forward!', 'Challenges are what make life interesting. Embrace them and stay motivated!', 'Believe in yourself, for you are capable of achieving incredible things!', 'Your dreams are within reach. Stay focused and motivated to turn them into reality.', "Don't be discouraged by setbacks. They are stepping stones to success. Keep going!"],
  'context_set': ''
},
{
  'tag': 'shakespeare_poems',
  'patterns': ['Read me a poem by Shakespeare.', 'Share a Shakespearean poem with me.', 'Let me hear a poem written by Shakespeare.'],
  'responses': ["Certainly! Here's a beautiful poem by William Shakespeare:\n\nOver hill, over dale,\nThorough bush, thorough brier,\nOver park, over pale,\nThorough flood, thorough fire!\nI do wander everywhere,\nSwifter than the moon's sphere;\nAnd I serve the Fairy Queen,\nTo dew her orbs upon the green;\nThe cowslips tall her pensioners be;\nIn their gold coats spots you see;\nThose be rubies, fairy favours;\nIn those freckles live their savours;\nI must go seek some dewdrops here,\nAnd hang a pearl in every cowslip's ear."],
  'context_set': ''}
  ]
}

# Sofia Starinnova

import nltk
import json
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import pickle
import os
import time
import matplotlib.pyplot as plt
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')

stemmer = WordNetLemmatizer()

words = []
classes = []
documents = []
ignore_words = ['?']


for intent in intents['intents']:
    for pattern in intent['patterns']:
        # Tokenize each word in the sentence
        w = nltk.word_tokenize(pattern)
        # Add to our words list
        words.extend(w)
        documents.append((w, intent['tag']))
        # Add to our classes list
        if intent['tag'] not in classes:
            classes.append(intent['tag'])


words = [stemmer.lemmatize(w.lower()) for w in words if w not in ignore_words]
words = sorted(list(set(words)))


classes = sorted(list(set(classes)))

print(len(documents), "documents")
print(len(classes), "classes", classes)
print(len(words), "unique lemmatized words", words)

# Sofia Starinnova

lemmatizer = WordNetLemmatizer()
exclude_words = {'?', '!', '.', ','}

# Preprocess data
def preprocess_data(intents):
    vocabulary = set()
    categories = set()
    training_data = []
    for intent in intents['intents']:
        categories.add(intent['tag'])
        for sentence in intent['patterns']:
            tokens = nltk.word_tokenize(sentence)
            vocabulary.update([lemmatizer.lemmatize(word.lower()) for word in tokens if word not in exclude_words])
            training_data.append((tokens, intent['tag']))
    return sorted(list(vocabulary)), sorted(list(categories)), training_data

vocabulary, categories, training_data = preprocess_data(intents)

# Preparing data
def prepare_data(vocabulary, categories, training_data):
    X, y = [], []
    empty_output = [0] * len(categories)
    for doc in training_data:
        bag_of_words = [1 if word in [lemmatizer.lemmatize(w.lower()) for w in doc[0]] else 0 for word in vocabulary]
        output_row = list(empty_output)
        output_row[categories.index(doc[1])] = 1
        X.append(bag_of_words)
        y.append(output_row)
    return np.array(X), np.array(y)

X_train, y_train = prepare_data(vocabulary, categories, training_data)

# Synonym Augmentation & Handling Multi-word Synonyms (Phrases)
def augment_sentence(sentence):
    synonyms = {
        "hi": ["hey", "hello there", "howdy", "hey there", "long time no see", "top of the morning", "pleasure to meet you"],
        "goodbye": ["see you", "later", "take care", "catch you later", "farewell", "until next time", "talk to you later"]
    }

    augmented_sentences = [sentence]

    for key, values in synonyms.items():
        for synonym in values:
            if synonym in sentence:
                new_sentence = sentence.replace(synonym, key)
                augmented_sentences.append(new_sentence)

    return augmented_sentences

# Generating augmented data
def generate_augmented_data(X_train, y_train, vocabulary):
    augmented_X, augmented_y = [], []
    for features, label in zip(X_train, y_train):
        tokens = [vocabulary[i] for i in range(len(vocabulary)) if features[i] == 1]
        augmented_sentences = augment_sentence(' '.join(tokens))
        for augmented_sentence in augmented_sentences:
            augmented_features = [0] * len(vocabulary)
            for word in augmented_sentence.split():
                if word in vocabulary:
                    augmented_features[vocabulary.index(word)] = 1
            augmented_X.append(augmented_features)
            augmented_y.append(label)
    return np.array(augmented_X), np.array(augmented_y)

augmented_X, augmented_y = generate_augmented_data(X_train, y_train, vocabulary)
combined_X = np.concatenate((X_train, augmented_X), axis=0)
combined_y = np.concatenate((y_train, augmented_y), axis=0)

train_X, test_X, train_Y, test_Y = train_test_split(combined_X, combined_y, test_size=0.2, random_state=42)

# Dataset class
class IntentDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __len__(self):
        return len(self.features)
    def __getitem__(self, idx):
        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)

train_dataset = IntentDataset(train_X, train_Y)
test_dataset = IntentDataset(test_X, test_Y)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Sofia Starinnova
# BiGRU-CNN model
class BiGRU_CNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(BiGRU_CNN, self).__init__()
        #self.quant = torch.quantization.QuantStub()#1
        self.gru_layer = nn.GRU(input_dim, hidden_dim, bidirectional=True, batch_first=True)
        self.conv_layer = nn.Conv1d(hidden_dim * 2, 64, kernel_size=3, padding=1)
        self.fc_layer = nn.Linear(64, output_dim)
        #self.dequant = torch.quantization.DeQuantStub()#1
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        #x = self.quant(x) #1
        x = x.unsqueeze(1)
        gru_out, _ = self.gru_layer(x)
        conv_out = self.conv_layer(gru_out.permute(0, 2, 1))
        pooled = torch.max(conv_out, dim=2)[0]
        output = self.fc_layer(pooled)
        return self.softmax(output)

model = BiGRU_CNN(input_dim=len(vocabulary), hidden_dim=128, output_dim=len(categories))
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters())

train_losses = []
test_losses = []
test_accuracies = []
total_times = []

# Training loop
best_acc=0
best_model_state=None
epochs = 70
epoch_start_time = time.time()
for epoch in range(epochs):
    model.train()
    train_loss = 0
    for inputs, targets in train_loader:
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    # Evaluating on test set
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, targets in test_loader:
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            test_loss += loss.item()
            correct += (torch.argmax(outputs, dim=1) == torch.argmax(targets, dim=1)).sum().item()
            total += targets.size(0)

    train_loss /= len(train_loader)
    test_loss /= len(test_loader)
    test_accuracy = correct / total

    train_losses.append(train_loss)
    test_losses.append(test_loss)
    test_accuracies.append(test_accuracy * 100)
    total_times.append(time.time() - epoch_start_time)

    if test_accuracy > best_acc:
        best_acc = test_accuracy
        best_model_state = model.state_dict()
torch.save(best_model_state, 'intent_model.pth')
print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%")



# Chang Lu

# Calculating total training time
end_time = time.time()
training_time = end_time - epoch_start_time
print(f"Total training time: {training_time:.2f} seconds")

# Saving vocabulary and categories
with open('vocabulary.pkl', 'wb') as f:
    pickle.dump(vocabulary, f)

with open('categories.pkl', 'wb') as f:
    pickle.dump(categories, f)

# Saving trained model
torch.save(model.state_dict(), 'intent_model.pth')

# Saving FAQ cache
faq_cache = {sentence: intent['tag'] for intent in intents['intents'] for sentence in intent['patterns']}
with open('faq_cache.pkl', 'wb') as f:
    pickle.dump(faq_cache, f)

#Post-Training Quantization (to reduce model size and inference time)
import torch.quantization
quantized_model = BiGRU_CNN(input_dim=len(vocabulary), hidden_dim=128, output_dim=len(categories))
quantized_model.load_state_dict(torch.load("intent_model.pth"))
quantized_model.eval()
# Apply dynamic quantization to GRU and Linear layers
quantized_model = torch.quantization.quantize_dynamic(
    quantized_model, {nn.GRU, nn.Linear}, dtype=torch.qint8)
# Save quantized model
torch.save(quantized_model.state_dict(), "intent_model_quantized.pth")
print("Quantized model saved as 'intent_model_quantized.pth'")

#Export the Model to ONNX (for mobile/web or TensorRT)
!pip install onnx onnxruntime
onnx_model = BiGRU_CNN(input_dim=len(vocabulary), hidden_dim=128, output_dim=len(categories))
onnx_model.load_state_dict(torch.load("intent_model.pth"))
onnx_model.eval()
# Dummy input for ONNX tracing
dummy_input = torch.randn(1, len(vocabulary))  # shape: [batch_size, input_dim]
# Export to ONNX file
torch.onnx.export(
    onnx_model,
    dummy_input,
    "intent_model.onnx",
    input_names=["input"],
    output_names=["output"],
    dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
    opset_version=11
)
print("Model exported to ONNX format as 'intent_model.onnx'")

# Chang Lu

import time
import torch

# load float32 model
model_fp32 = BiGRU_CNN(input_dim=len(vocabulary), hidden_dim=128, output_dim=len(categories))
model_fp32.load_state_dict(torch.load("intent_model.pth"))
model_fp32.eval().to("cpu")

# load int8 quan model
model_int8 = BiGRU_CNN(input_dim=len(vocabulary), hidden_dim=128, output_dim=len(categories))
model_int8.load_state_dict(torch.load("intent_model.pth"))
model_int8 = torch.quantization.quantize_dynamic(model_int8, {nn.GRU, nn.Linear}, dtype=torch.qint8)
model_int8.eval().to("cpu")

dummy_input = torch.randn(1, len(vocabulary)).to("cpu")

# measure inference time
def measure_inference_time(model, inputs, repeat=100):
    with torch.no_grad():
        start = time.time()
        for _ in range(repeat):
            _ = model(inputs)
        end = time.time()
    return (end - start) / repeat * 1000

fp32_time = measure_inference_time(model_fp32, dummy_input)
int8_time = measure_inference_time(model_int8, dummy_input)

print(f"Float32 Inference time: {fp32_time:.4f} ms")
print(f"INT8 time: {int8_time:.4f} ms")

# Inference time comparison
plt.figure(figsize=(8, 5))
plt.bar(["Float32", "Quantized (INT8)"], [fp32_time, int8_time], color=["skyblue", "lightgreen"])
plt.ylabel("Average Inference Time (ms)")
plt.title("Inference Time Comparison")
plt.grid(True, axis='y')
plt.show()

# Chang Lu

float_acc = max(test_accuracies)
quantized_acc = correct / total * 100

# Accuracy comparison
plt.figure(figsize=(6, 4))
plt.bar(["Float32", "Quantized (INT8)"], [float_acc, quantized_acc], color=["orange", "purple"])
plt.title("Model Accuracy Comparison (Float32 vs INT8)")
plt.ylabel("Accuracy (%)")
plt.ylim(0, 100)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

#Visualization: original float32 model size VS Quantized int8 model size
# original float32 model
torch.save(model.state_dict(), "float_model.pth")
quantized_model = BiGRU_CNN(input_dim=len(vocabulary), hidden_dim=128, output_dim=len(categories))
quantized_model.load_state_dict(torch.load("intent_model.pth"))
quantized_model.eval()
# Apply dynamic quantization to GRU and Linear layers
quantized_model = torch.quantization.quantize_dynamic(
    quantized_model, {nn.GRU, nn.Linear}, dtype=torch.qint8)
# Save quantized model
torch.save(quantized_model.state_dict(), "intent_model_quantized.pth")
print("Quantized model saved as 'intent_model_quantized.pth'")

# Quantized int8 model
torch.save(quantized_model.state_dict(), "quant_model.pth")

size_float = os.path.getsize("float_model.pth") / 1e6
size_quant = os.path.getsize("quant_model.pth") / 1e6

plt.bar(["Float32", "Quantized (INT8)"], [size_float, size_quant], color=["skyblue", "lightgreen"])
plt.title("Model Size After Quantization")
plt.ylabel("Size (MB)")
plt.show()

# Sofia Starinnova

# Plotting training loss, testing loss, and accuracy
#epochs_range = range(1, epochs + 1)
epochs_range = range(len(train_losses))

# Loss Graph
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, train_losses, label='Train Loss')
plt.plot(epochs_range, test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train vs Test Loss')
plt.legend()
plt.grid(True)

# Accuracy Graph
plt.subplot(1, 2, 2)
plt.plot(epochs_range, test_accuracies, label='Test Accuracy', color='green')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Test Accuracy')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Sofia Starinnova

import matplotlib.pyplot as plt
import numpy as np

# Final values after training
final_train_loss = train_losses[-1]
final_test_loss = test_losses[-1]
final_test_accuracy = test_accuracies[-1]

# Labels and values for the bar chart
metrics = ['Train Loss', 'Test Loss', 'Test Accuracy']
values = [final_train_loss, final_test_loss, final_test_accuracy]

colors = ['blue', 'red', 'green']

plt.figure(figsize=(8, 5))
plt.bar(metrics, values, color=colors)

for i, v in enumerate(values):
    plt.text(i, v + 0.01, f"{v:.2f}", ha='center', fontsize=12, fontweight='bold')

# Labels and title
plt.ylabel("Value")
plt.title("Final Training Metrics")
plt.ylim(0, max(values) * 1.2)

# Plot
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Sofia Starinnova
# Chatbot

import random
import re
import time
import ipywidgets as widgets
from IPython.display import display

def replace_synonyms(message):
    synonyms = {
        "hi": ["hey", "hello there", "howdy", "hey there", "long time no see", "top of the morning", "pleasure to meet you"],
        "goodbye": ["see you", "later", "take care", "catch you later", "farewell", "until next time", "talk to you later"]
    }

    message_lower = message.lower()

    for key, values in synonyms.items():
        for synonym in values:
            # Replace whole phrases to their base synonym
            message_lower = re.sub(r'\b' + re.escape(synonym) + r'\b', key, message_lower, flags=re.IGNORECASE)

    return message_lower

def chatbot_output(message):
    start_time = time.time()

    # Normalize synonyms before processing
    message = replace_synonyms(message)

    response = "I'm sorry, I didn't quite understand that. Can you ask something else?"

    # Check each intent separately
    for intent in intents['intents']:
        for pattern in intent['patterns']:
            if re.search(pattern.lower(), message, re.IGNORECASE):  # Change from `fullmatch` to `search`
                response = random.choice(intent['responses'])  # Pick a random response if multiple exist
                break

    response_time = time.time() - start_time
    return response, response_time

# Display chat history with a scrollbar
chat_output = widgets.Output(
    layout={
        'border': '1px solid black',
        'width': '400px',
        'height': '400px',
        'overflow_y': 'auto'
    }
)

# Textarea for the user to type their message
input_box = widgets.Textarea(placeholder='Type your message...', layout={'width': '80%', 'height': '60px'})

# Button to send message
send_button = widgets.Button(description="Send", button_style='primary')

# Stores real response times
response_times = []

def send_button_click(b):
    global response_times
    # Convert to lowercase
    message = input_box.value.strip().lower()
    if message == "quit":
        with chat_output:
            print("You: quit\n")
            print("Bot: Goodbye! Exiting chat...\n")
        return  # Exits the function
    if message != '':
        with chat_output:
            print("You: " + message + '\n')

        # Get chatbot response and response time
        res, response_time = chatbot_output(message)

        # Stores real response time
        response_times.append(response_time)

        with chat_output:
            print(f"Bot: {res} (Response time: {response_time:.4f} seconds)\n")

        # Clear the input box after sending the message
        input_box.value = ''

# Clicking on button will send the message
send_button.on_click(send_button_click)

# Display chat output, input box, and send button
display(chat_output, input_box, send_button)

# Sofia Starinnova
# Visualization of Response Times

import numpy as np
import matplotlib.pyplot as plt

# 1. Line Plot of Response Times
def plot_response_times():
    if not response_times:
        print("No response times to plot.")
        return

    plt.plot(range(len(response_times)), response_times, marker='o', linestyle='-', label="Response Time")
    plt.xlabel("Interaction Number")
    plt.ylabel("Response Time (seconds)")
    plt.title("Chatbot Response Times Over Interactions")
    plt.legend()
    plt.grid(True)
    plt.show()

# 2. Overall Average Response Time
def plot_average_response_time():
    if not response_times:
        print("No response times to plot.")
        return

    # Overall average
    average_response_time = np.mean(response_times)

    # Scatter plot of individual response times
    plt.scatter(range(len(response_times)), response_times, color='b', alpha=0.6, label="Response Times")

    # Plotting the Overall Average Response time (as a red dashed line)
    plt.axhline(y=average_response_time, color='r', linestyle='--', label=f"Avg Response Time")

    plt.xlabel("Interaction Number")
    plt.ylabel("Response Time (seconds)")
    plt.title("Overall Average Response Time")
    plt.legend()
    plt.grid(True)
    plt.show()

plot_response_times()
plot_average_response_time()

# Chang Lu

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import nltk
from wordcloud import WordCloud

# Visualization 1: Number of Patterns per Intent
intent_tags = [intent["tag"] for intent in intents["intents"]]
pattern_counts = [len(intent["patterns"]) for intent in intents["intents"]]
plt.figure(figsize=(8, 5))
plt.bar(intent_tags, pattern_counts, color='skyblue')
plt.xlabel('Intent Categories')
plt.ylabel('Number of Patterns')
plt.title('Number of Patterns per Intent')
plt.xticks(rotation=45)
plt.show()

# Visualization 2: Word Cloud of Patterns
all_patterns = [pattern for intent in intents["intents"] for pattern in intent["patterns"]]
text = " ".join(all_patterns)
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Patterns")
plt.show()

# Visualization 3: Top N Most Frequent Words
word_counts = {}
for sentence in all_patterns:
    for word in nltk.word_tokenize(sentence):
        word = word.lower()
        word_counts[word] = word_counts.get(word, 0) + 1
sorted_words = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)
top_n = 20
plt.figure(figsize=(10, 6))
plt.bar([word[0] for word in sorted_words[:top_n]], [word[1] for word in sorted_words[:top_n]])
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title(f'Top {top_n} Most Frequent Words')
plt.xticks(rotation=45)
plt.show()

# Visualization 4: Distribution of Labels
label_counts = {label: sum(1 for doc in training_data if doc[1] == label) for label in categories}
plt.figure(figsize=(8, 5))
plt.bar(label_counts.keys(), label_counts.values())
plt.xlabel('Labels')
plt.ylabel('Frequency')
plt.title('Distribution of Labels')
plt.xticks(rotation=45)
plt.show()

# Visualization 5: Heatmap of Pattern Lengths
pattern_lengths = [[len(pattern.split()) for pattern in intent["patterns"]] for intent in intents["intents"]]
intent_labels = [intent["tag"] for intent in intents["intents"]]
max_len = max(map(len, pattern_lengths))
padded_data = np.array([row + [np.nan] * (max_len - len(row)) for row in pattern_lengths])
plt.figure(figsize=(10, 5))
sns.heatmap(padded_data, annot=True, cmap="Blues", xticklabels=False, yticklabels=intent_labels)
plt.xlabel("Pattern Index")
plt.ylabel("Intent Category")
plt.title("Heatmap of Pattern Lengths (Words per Pattern)")
plt.show()

# Visualization 6: Box Plot of Response Lengths Across Intents
response_data = [{"Intent": intent["tag"], "Response Length": len(response.split())}
                 for intent in intents["intents"] for response in intent["responses"]]
df = pd.DataFrame(response_data)
plt.figure(figsize=(10, 5))
sns.boxplot(x="Intent", y="Response Length", data=df, palette="coolwarm")
plt.xticks(rotation=45)
plt.title("Box Plot of Response Lengths Across Intents")
plt.show()

# Visualization 7: Comparison of Patterns and Responses per Intent
response_counts = [len(intent["responses"]) for intent in intents["intents"]]
x = np.arange(len(intent_tags))
width = 0.6
plt.figure(figsize=(10, 6))
plt.bar(x, pattern_counts, width, label="Patterns", color="dodgerblue")
plt.bar(x, response_counts, width, bottom=pattern_counts, label="Responses", color="salmon")
plt.xlabel("Intent Categories")
plt.ylabel("Count")
plt.title("Comparison of Patterns and Responses per Intent")
plt.xticks(x, intent_tags, rotation=45)
plt.legend()
plt.show()

# Visualization 8: Histogram of Pattern Lengths
pattern_lengths_flat = [len(pattern.split()) for intent in intents["intents"] for pattern in intent["patterns"]]
plt.figure(figsize=(10, 5))
plt.hist(pattern_lengths_flat, bins=np.arange(1, max(pattern_lengths_flat) + 2) - 0.5, color="royalblue", edgecolor="black")
plt.xlabel("Number of Words in Patterns")
plt.ylabel("Frequency")
plt.title("Histogram of Pattern Lengths")
plt.xticks(range(1, max(pattern_lengths_flat) + 1))
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

# Visualization 9: Scatter Plot of Pattern vs. Response Lengths
data = [{"Intent": intent["tag"], "Pattern Length": len(pattern.split()), "Response Length": len(response.split())}
        for intent in intents["intents"] for pattern in intent["patterns"] for response in intent["responses"]]
df = pd.DataFrame(data)
plt.figure(figsize=(12, 6))
sns.scatterplot(x="Pattern Length", y="Response Length", hue="Intent", data=df, palette="tab10", alpha=0.7)
plt.xlabel("Pattern Length (Words)")
plt.ylabel("Response Length (Words)")
plt.title("Scatter Plot of Pattern vs. Response Lengths")
plt.grid(True, linestyle="--", alpha=0.7)
plt.show()

"""# Our Observation"""

# We decided to do many different types of visualizations to show the chatbot's functionality and improvements.